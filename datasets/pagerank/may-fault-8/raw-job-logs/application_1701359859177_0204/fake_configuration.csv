,name,value,description
0,dfs.namenode.snapshotdiff.listing.limit,1709988508,"Limit the number of entries generated by getSnapshotDiffReportListing within
    one rpc call to the namenode.If less or equal to zero, at most
    DFS_NAMENODE_SNAPSHOT_DIFF_LISTING_LIMIT_DEFAULT (= 1000) will be sent
    across to the client within one rpc call."
1,yarn.client.failover-retries-on-socket-timeouts,0,
2,yarn.resourcemanager.connect.max-wait.ms,900000,
3,dfs.namenode.startup.delay.block.deletion.sec,0,"The delay in seconds at which we will pause the blocks deletion
    after Namenode startup. By default it's disabled.
    In the case a directory has large number of directories and files are
    deleted, suggested delay is one hour to give the administrator enough time
    to notice large number of pending deletion blocks and take corrective
    action."
4,yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size,10000,
5,hadoop.hdfs.configuration.version,1,version of this configuration file
6,dfs.namenode.num.extra.edits.retained,1000000,"The number of extra transactions which should be retained
  beyond what is minimally necessary for a NN restart.
  It does not translate directly to file's age, or the number of files kept,
  but to the number of transactions (here ""edits"" means transactions).
  One edit file may contain several transactions (edits).
  During checkpoint, NameNode will identify the total number of edits to retain as extra by
  checking the latest checkpoint transaction value, subtracted by the value of this property.
  Then, it scans edits files to identify the older ones that don't include the computed range of
  retained transactions that are to be kept around, and purges them subsequently.
  The retainment can be useful for audit purposes or for an HA setup where a remote Standby Node may have
  been offline for some time and need to have a longer backlog of retained
  edits in order to start again.
  Typically each edit is on the order of a few hundred bytes, so the default
  of 1 million edits should be on the order of hundreds of MBs or low GBs.

  NOTE: Fewer extra edits may be retained than value specified for this setting
  if doing so would mean that more segments would be retained than the number
  configured by dfs.namenode.max.extra.edits.segments.retained."
7,mapreduce.map.skip.maxrecords,0,"The number of acceptable skip records surrounding the bad
    record PER bad record in mapper. The number includes the bad record as well.
    To turn the feature of detection/skipping of bad records off, set the
    value to 0.
    The framework tries to narrow down the skipped range by retrying
    until this threshold is met OR all attempts get exhausted for this task.
    Set the value to Long.MAX_VALUE to indicate that framework need not try to
    narrow down. Whatever records(depends on application) get skipped are
    acceptable."
8,yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size,10485760,
9,dfs.disk.balancer.max.disk.errors,5,"During a block move from a source to destination disk, we might
      encounter various errors. This defines how many errors we can tolerate
      before we declare a move between 2 disks (or a step) has failed."
