,name,value,description
0,io.bytes.per.checksum,,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.namenode.fs-limits.max-xattrs-per-inode,32,Maximum number of extended attributes per inode.
2,mapreduce.job.end-notification.retry.attempts,0,"The number of times the submitter of the job wants to retry job
    end notification if it fails. This is capped by
    mapreduce.job.end-notification.max.attempts"
3,dfs.client.block.write.retries,3,"The number of retries for writing blocks to the data nodes, 
  before we signal failure to the application."
4,file.blocksize,67108864,Block size
5,dfs.disk.balancer.max.disk.errors,5,"During a block move from a source to destination disk, we might
      encounter various errors. This defines how many errors we can tolerate
      before we declare a move between 2 disks (or a step) has failed."
6,mapreduce.task.skip.start.attempts,2,"The number of Task attempts AFTER which skip mode
    will be kicked off. When skip mode is kicked off, the
    tasks reports the range of records which it will process
    next, to the MR ApplicationMaster. So that on failures, the MR AM
    knows which ones are possibly the bad records. On further executions,
    those are skipped."
7,hadoop.security.groups.negative-cache.secs,30,"Expiration time for entries in the the negative user-to-group mapping
    caching, in seconds. This is useful when invalid users are retrying
    frequently. It is suggested to set a small value for this expiration, since
    a transient error in group lookup could temporarily lock out a legitimate
    user.

    Set this to zero or negative value to disable negative user-to-group caching."
8,mapreduce.shuffle.pathcache.concurrency-level,16,"Uses the concurrency level to create a fixed number of hashtable
    segments, each governed by its own write lock."
9,dfs.namenode.replication.max-streams-hard-limit,4,Hard limit for all replication streams.
