,name,value,description
0,io.bytes.per.checksum,,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.datanode.socket.reuse.keepalive,4000,"The window of time in ms before the DataXceiver closes a socket for a
    single request.  If a second request occurs within that window, the
    socket can be reused."
2,ipc.[port_number].weighted-cost.lockexclusive,100,"The weight multiplier to apply to the time spent in the
    processing phase which holds an exclusive (write) lock.
    This property applies to WeightedTimeCostProvider."
3,dfs.http.client.retry.max.attempts,10,"Specify the max number of retry attempts for WebHDFS client,
    if the difference between retried attempts and failovered attempts is
    larger than the max number of retry attempts, there will be no more
    retries."
4,yarn.resourcemanager.activities-manager.cleanup-interval-ms,5000,
5,hadoop.security.groups.cache.background.reload.threads,3,"Only relevant if hadoop.security.groups.cache.background.reload is true.
    Controls the number of concurrent background user->group cache entry
    refreshes. Pending refresh requests beyond this value are queued and
    processed when a thread is free."
6,dfs.client.max.block.acquire.failures,3,Maximum failures allowed when trying to get block information from a specific datanode.
7,yarn.sharedcache.cleaner.initial-delay-mins,10,
8,yarn.nodemanager.disk-health-checker.interval-ms,120000,
9,dfs.http.client.failover.sleep.max.millis,15000,"Specify the upper bound of sleep time in milliseconds between
    retries or failovers for WebHDFS client."
